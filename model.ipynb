{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import image\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1'\n",
    "                ]\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    \"\"\"\n",
    "    this function will load pretrained(imagenet) vgg19 model and give access to output of intermedia layer\n",
    "    then it will initialize a new model which take a picture as a input and output a list of vgg19 layer output.\n",
    "    \n",
    "    Return:\n",
    "    return a model that input a picture and output the content feature and style feature\n",
    "    \"\"\"\n",
    "    \n",
    "    vgg19 = tf.keras.applications.vgg19(include_top = False,weights = 'imagenet')\n",
    "    vgg19.trainable = False\n",
    "    \n",
    "    content_outputs = [vgg19.get_layer(layername) for layername in content_layers]\n",
    "    style_outputs = [vgg19.get_layer(layername) for layername in style_layers]\n",
    "    model_outputs = content_outputs + style_outputs\n",
    "    \n",
    "    model = Model(vgg.input,model_outputs)\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base_content,target):\n",
    "    c_loss = tf.reduce_sum(tf.square(base_content - target))/2\n",
    "\n",
    "def gram_matrix(input_tensor):\n",
    "    channel = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor,[-1,channel]) # reshape img into dims of [H*W,C]\n",
    "    gram = tf.matmul(a,tf.transpose(a))\n",
    "    return gram\n",
    "\n",
    "def style_loss(base_style,target):\n",
    "    a = gram_matrix(base_style)\n",
    "    b = gram_matrix(target)\n",
    "    h,w,c = base_style.get_shape().as_list()\n",
    "    s_loss = tf.reduce_sum(tf.square(a - b))/(4*(h**2)*(w**2)*(c**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_representation(model,path,mode):\n",
    "    \"\"\"\n",
    "    This helper function will load  picture and the model, \n",
    "    then produce different layer output according to the need\n",
    "    \n",
    "    Arguments:\n",
    "     model : the model we initialized\n",
    "     path : path to the picture\n",
    "     mode : 'content' feature or 'style' feature\n",
    "     \n",
    "    Returns:\n",
    "     return the content features or the style features\n",
    "    \"\"\"\n",
    "    img = image.pre_process_img(path)\n",
    "    feature_outputs = model(img)\n",
    "    if mode == 'style':\n",
    "        return [feature[0] for feature in feature_outputs[num_content_layers:]] # get the last several outputs\n",
    "    if mode =='content':\n",
    "        return [feature[0] for feature in feature_outputs[:num_content_layers]] # get the first several outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model,loss_weights,init_image,content_features,style_features):\n",
    "    \"\"\"\n",
    "    The loss function\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "     model : the model we are using\n",
    "     loss_weights : the weights of each contribution in the loss function\n",
    "                     (conten tloss weight,style loss weight,vatiation weight)\n",
    "     init_image : the generated image upon which we would impose Gradient Descent\n",
    "     content_features : the precomputed content picture feature\n",
    "     style_features : the precomputed style picture feature\n",
    "     \n",
    "    Returns:\n",
    "    \n",
    "     the total loss we are going to optimize\n",
    "    \"\"\"\n",
    "    style_weight,content_weight = loss_weights\n",
    "    \n",
    "    # feed the init image in the model,then we would get the \n",
    "    # content feature and the style feature from the layers \n",
    "    # we desire\n",
    "    \n",
    "    features = model(init_image)\n",
    "    gen_style_feature = features[num_content_layers:]\n",
    "    gen_content_feature = features[:num_content_layers]\n",
    "    \n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    \n",
    "    # equal seperate the layer weight of style loss\n",
    "    # and accumulate the style loss from desired layers\n",
    "    weight_per_style_layer = 1.0/ float(num_style_layers)\n",
    "    for style_pic_features,gen_pic_stylefeatures in zip(style_features,gen_style_feature):\n",
    "        style_loss += weight_per_style_layer * style_loss(style_pic_features,gen_pic_stylefeatures)\n",
    "    \n",
    "    # equal seperate the layer weight of content loss\n",
    "    # and accumulate the content loss from desired layers\n",
    "    weight_per_content_layer = 1.0/ float(num_content_layers)\n",
    "    for content_pic_features,gen_pic_contentfeatures in zip(content_features,gen_content_feature):\n",
    "        content_loss += weight_per_content_layer * content_loss(content_pic_features,gen_pic_contentfeatures)\n",
    "    \n",
    "    style_loss *= style_weight\n",
    "    content_loss *= content_weight\n",
    "    total_loss = style_loss + content_loss\n",
    "    return total_loss,content_loss,style_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss(**cfg)\n",
    "    #Compute Gradient with respect to the generated image\n",
    "    total_loss = loss[0]\n",
    "    return tape.gradient(total_loss,cfg['init_image']),loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nst(content_path,style_path,iteration = 1000,content_weight = 1e-3,style_weight = 1):\n",
    "    model = model_init()\n",
    "    for layer in model:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    content_features = get_feature_representation(model,content_path,mode = 'content')\n",
    "    style_features = get_feature_representation(model,style_path,mode = 'style')\n",
    "    \n",
    "    init_image = image.pre_process_img(content_path) # initialize the generated image with content image\n",
    "    init_image = tf.Variable(init_image,dtype = tf.float32)\n",
    "    \n",
    "    opt = tf.train.AdamOptimizer(5,beta1 = 0.99,beta2 = 1e-1)\n",
    "    \n",
    "    epoch = 1\n",
    "    loss_weights = (content_weight,style_weight)\n",
    "    \n",
    "    cfg = {\n",
    "        'model':model,\n",
    "        'loss_weights':loss_weights,\n",
    "        'init_image':init_image,\n",
    "        'content_features':content_features,\n",
    "        'style_features':style_features\n",
    "    }\n",
    "    \n",
    "    norm_means = np.array([103.939, 116.779, 123.68])\n",
    "    min_vals = -norm_means\n",
    "    max_vals = 255 - norm_means\n",
    "    \n",
    "    #store the loss and the img\n",
    "    best_loss, best_img = float('inf'), None\n",
    "    imgs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, all_loss = compute_grads(cfg)\n",
    "        loss, content_loss, style_loss = all_loss\n",
    "        opt.apply_gradients([(grads, init_image)])\n",
    "        clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "        init_image.assign(clipped)\n",
    "\n",
    "        if loss < best_loss:\n",
    "            # Update best loss and best image from total loss.\n",
    "            best_loss = loss\n",
    "            best_img = image.deprocess_img(init_image.numpy())\n",
    "\n",
    "        if i % display_interval == 0:\n",
    "            # Use the .numpy() method to get the concrete numpy array\n",
    "            plot_img = init_image.numpy()\n",
    "            plot_img = image.deprocess_img(plot_img)\n",
    "\n",
    "            path = 'output/output_' + str(i) + '.jpg'\n",
    "\n",
    "            image.save_results(plot_img, path)\n",
    "            imgs.append(plot_img)\n",
    "\n",
    "            print('Iteration: {}'.format(i))\n",
    "            print('Total loss: {:.4e}, '\n",
    "                  'style loss: {:.4e}, '\n",
    "                  'content loss: {:.4e}'\n",
    "                  .format(loss, style_score, content_score))\n",
    "\n",
    "    return best_img, best_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
